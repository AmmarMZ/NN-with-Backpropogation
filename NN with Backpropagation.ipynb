{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "import math\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function\n",
    "import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = \"data/ex4data1.mat\"\n",
    "data = scipy.io.loadmat(dataFile)\n",
    "\n",
    "\n",
    "# X has 5000 examples and 400 features making it 5000x400\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# training set / test set code\n",
    "# shuffling data set because later on pd.get_dummies needs to identify 10 unique elements but can't if the data only contains \n",
    "# classifications for the first 6 digits\n",
    "combined = np.append(X, y, axis=1)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "yComb = combined[:,400]\n",
    "yComb = yComb.reshape(len(yComb),1)\n",
    "\n",
    "Xcomb = combined[:,0:400]\n",
    "Xcomb = Xcomb.reshape(len(Xcomb),400)\n",
    "\n",
    "# will be using 70% of data to train hence 3500 x 400\n",
    "Xtrain = Xcomb[0:3500,:]\n",
    "Xtrain = np.insert(Xtrain, 0, 1, axis=1)\n",
    "yTrain = yComb[0:3500]\n",
    "\n",
    "# then the test set will be 1500x400\n",
    "Xtest = Xcomb[3500:5001,:]\n",
    "yTest = yComb[3500:5000]\n",
    "\n",
    "#end of train/test set code\n",
    "\n",
    "X = np.insert(X, 0, 1, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from ex3\n",
    "def reshapeRow(row):\n",
    "    \"\"\"\n",
    "    @param {row} 1 x 401 matrix since an image of a digit is 20x20 + 1 that was added as a bias\n",
    "    Function that takes in the pixel intensity values and puts it into a 20x20 square \n",
    "    \"\"\" \n",
    "    # the [1:] is used to take everything after the 1st index \n",
    "    \n",
    "    return row[1:].reshape(20,20).T\n",
    "\n",
    "def displayData(indiciesToDisplay = None):\n",
    "    \"\"\"\n",
    "    Function that selects 100 random examples for the 5000 we have and organizes\n",
    "    them into a 10x10 matrix\n",
    "    \"\"\"\n",
    "    width = 20\n",
    "    height = 20\n",
    "    numRows = 10\n",
    "    numCols = 10\n",
    "    \n",
    "    if not indiciesToDisplay:\n",
    "        indiciesToDisplay = random.sample(range(X.shape[0]), numRows * numCols)\n",
    "\n",
    "    \n",
    "    bigPicture = np.zeros((height * numRows, width * numCols))\n",
    "    \n",
    "    iRow = 0\n",
    "    iCol = 0\n",
    "\n",
    "    for i in indiciesToDisplay :\n",
    "        if iCol == numCols:\n",
    "            iCol = 0\n",
    "            iRow += 1\n",
    " \n",
    "        curImg = reshapeRow(X[i])\n",
    "        bigPicture[iRow * height :iRow * height + curImg.shape[0], \n",
    "                    iCol * width : iCol * width + curImg.shape[1]] = curImg\n",
    "        iCol += 1 \n",
    "    fig = plt.figure( figsize = (6,6) )\n",
    "    img = scipy.misc.toimage( bigPicture )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "\n",
    "#displayData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFile = \"data/ex4weights.mat\"\n",
    "thetas1 = scipy.io.loadmat(thetaFile)\n",
    "\n",
    "# Theta1.shape = 25 x 401\n",
    "Theta1 = thetas1['Theta1']\n",
    "\n",
    "# Theta2.shape = 10 x 26\n",
    "Theta2 = thetas1['Theta2']\n",
    "\n",
    "thetaUnrolled = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "\n",
    "inputSize = 400\n",
    "hiddenSize = 25\n",
    "outputSize = 10\n",
    "\n",
    "\n",
    "# m = 5000\n",
    "m = X.shape[0]\n",
    "# n = 401\n",
    "n = X.shape[1]\n",
    "\n",
    "aVals = [None] * 3\n",
    "zVals = [None] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following 2 functions are needed because fmin_cg passes in X as an unrolled vector\n",
    "def reshape(X, m, n):\n",
    "    return np.array(X).reshape((m,n))\n",
    "\n",
    "def flatten(X):\n",
    "    return np.array(X.flatten()).reshape((X.shape[0]*(X.shape[1]),1))\n",
    "\n",
    "\n",
    "# used to flatted D1, D2, taken from kaleko github\n",
    "def flattenParams(thetas_list):\n",
    "    input_layer_size = 400\n",
    "    hidden_layer_size = 25\n",
    "    output_layer_size = 10\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def sigmoid(X, theta):\n",
    "    return expit(np.dot(X,theta))\n",
    "\n",
    "def costFunction(thetas, X, y, lmbda):\n",
    "    \n",
    "    X = reshape(X,3500,401)\n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    m = X.shape[0]\n",
    "\n",
    "    a1 = X # 5000 x 401\n",
    "    z2 = theta1.dot(X.T)\n",
    "    \n",
    "    a2 = expit(z2) # 25 x 5000\n",
    "    a2 = np.insert(a2,0,1,axis=0)\n",
    "       \n",
    "    z3 = theta2.dot(a2)\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "    aVals[0] = a1\n",
    "    aVals[1] = a2\n",
    "    aVals[2] = a3\n",
    "    \n",
    "    zVals[0] = z2\n",
    "    zVals[1] = z3\n",
    "     \n",
    "    tempY = pd.get_dummies(y.ravel()).values\n",
    "    \n",
    "    J = (-1/m)*np.sum(np.log(a3.T)*(tempY)+np.log(1-a3).T*(1-tempY)) + \\\n",
    "    (lmbda/(2*m))*(np.sum(np.square(theta1[:,1:])) + np.sum(np.square(theta2[:,1:])))  \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCost():\n",
    "    J = costFunction(thetaUnrolled,Xtrain,yTrain,1)\n",
    "    #print(\"Expected value for training set is ~ 1.83\\nActual value %.9f\"%J)\n",
    "\n",
    "    #print(\"Expected value is ~ 0.383\\nActual value %.9f\"%J)\n",
    "testCost()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientSigmoid(z):\n",
    "    return expit(z)*(1-expit(z))\n",
    "\n",
    "def initRandomThetas():\n",
    "    epsilon = 0.12\n",
    "    theta1Size = (hiddenSize, inputSize + 1)\n",
    "    theta2Size = (outputSize, hiddenSize + 1)\n",
    "    \n",
    "    \n",
    "    theta1 = np.random.rand(*theta1Size).ravel()*2*epsilon - epsilon\n",
    "    theta2 = np.random.rand(*theta2Size).ravel()*2*epsilon - epsilon\n",
    "    \n",
    "    \n",
    "    thetas = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "    return thetas\n",
    "\n",
    "temp = initRandomThetas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 1114. Numerical Gradient = 0.000190. BackProp Gradient = 0.000190.\n",
      "Element: 2643. Numerical Gradient = 0.000524. BackProp Gradient = 0.000524.\n",
      "Element: 1048. Numerical Gradient = -0.000631. BackProp Gradient = -0.000631.\n",
      "Element: 1811. Numerical Gradient = 0.000235. BackProp Gradient = 0.000235.\n",
      "Element: 1550. Numerical Gradient = -0.000114. BackProp Gradient = -0.000114.\n",
      "Element: 743. Numerical Gradient = -0.000000. BackProp Gradient = -0.000000.\n",
      "Element: 1104. Numerical Gradient = 0.000008. BackProp Gradient = 0.000008.\n",
      "Element: 2779. Numerical Gradient = -0.000068. BackProp Gradient = -0.000068.\n",
      "Element: 3354. Numerical Gradient = -0.000456. BackProp Gradient = -0.000456.\n",
      "Element: 1802. Numerical Gradient = 0.000069. BackProp Gradient = 0.000069.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def backProp(thetas,X,y, lmbda):\n",
    "    X = reshape(X,3500,401)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    a1 = aVals[0]\n",
    "    a2 = aVals[1].T\n",
    "    a3 = aVals[2].T\n",
    "    \n",
    "    z2 = zVals[0].T\n",
    "    z3 = zVals[1].T\n",
    "    \n",
    "        \n",
    "    D1 = np.zeros((theta1.shape))\n",
    "    D2 = np.zeros((theta2.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1Cur = X[i]\n",
    "        a2Cur = a2[:,1:][i]\n",
    "        z2Cur = z2[i]\n",
    "        a3Cur = a3[i]\n",
    "        z3Cur = z3[i]\n",
    "    \n",
    "        a1Cur = a1Cur.reshape(len(a1Cur),1)\n",
    "        a2Cur = a2Cur.reshape(len(a2Cur),1)\n",
    "        z2Cur = z2Cur.reshape(len(z2Cur),1)\n",
    "        a3Cur = a3Cur.reshape(len(a3Cur),1)\n",
    "        z3Cur = z3Cur.reshape(len(z3Cur),1)\n",
    "        \n",
    "        yT = np.zeros((10,1))\n",
    "        yT[int(y[i]) - 1] = 1\n",
    "                \n",
    "        delta3 = a3Cur - yT\n",
    "        delta2 = theta2[:,1:].T.dot(delta3)*gradientSigmoid(z2Cur)\n",
    "        \n",
    "        \n",
    "        a2Cur = np.insert(a2Cur, 0, 1, axis=0)\n",
    "\n",
    "        D1 = D1 + delta2.dot(a1Cur.T)\n",
    "                \n",
    "        delta3 = delta3.reshape(len(delta3),1)\n",
    "        D2 = D2 + delta3.dot(a2Cur.T)\n",
    "        \n",
    "    D1 = D1/m\n",
    "    D2 = D2/m\n",
    "    \n",
    "    D1[:,1:] = D1[:,1:] + (float(lmbda)/m)*theta1[:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (float(lmbda)/m)*theta2[:,1:]\n",
    "    \n",
    "    return flattenParams([D1, D2]).flatten()\n",
    "       \n",
    "def checkGradient(thetas,D,X,y,lmbda):\n",
    "    epsilon = 0.0001\n",
    "    xT = flatten(X)\n",
    "    n_elems = len(thetas) \n",
    "    thetas = thetas.reshape(len(thetas),1)\n",
    "    #Pick ten random elements, compute numerical gradient, compare to respective D's\n",
    "    for i in range(10):\n",
    "        x = int(np.random.rand()*X.shape[0])\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        epsvec[x] = epsilon\n",
    "        cost_high = costFunction(thetas + epsvec, X, y, lmbda)\n",
    "        cost_low  = costFunction(thetas - epsvec, X, y, lmbda)\n",
    "        mygrad = (cost_high - cost_low) / float(2*epsilon)\n",
    "        print (\"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"%(x,mygrad,D[x]))\n",
    "        \n",
    "\n",
    "Ds = backProp(np.r_[Theta1.ravel(),Theta2.ravel()],Xtrain,yTrain,0)\n",
    "checkGradient(thetaUnrolled,Ds,Xtrain,yTrain,0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.024044\n",
      "         Iterations: 50\n",
      "         Function evaluations: 137\n",
      "         Gradient evaluations: 137\n",
      "Total time is 34.94 seconds\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "import time\n",
    "def trainNN(lmbda):\n",
    "    generatedThetas = initRandomThetas()\n",
    "    generatedThetas = generatedThetas.reshape(len(generatedThetas),1)\n",
    "    \n",
    "    result = scipy.optimize.fmin_cg(costFunction, fprime=backProp, x0=generatedThetas, \\\n",
    "                               args=(Xtrain,yTrain,lmbda),maxiter=50,disp=True,full_output=True)\n",
    "    return result[0]\n",
    "\n",
    "start = time.time()\n",
    "trainThetas = trainNN(0)\n",
    "end = time.time()\n",
    "print(\"Total time is %.2f seconds\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy is 95.07\n"
     ]
    }
   ],
   "source": [
    "def predictNN(row, index):\n",
    "    classes = np.arange(1,11) \n",
    "    a3 = aVals[2].T\n",
    "    a3Row = a3[index]\n",
    "    rVal = classes[np.argmax(a3Row)]\n",
    "    return rVal \n",
    "    \n",
    "\n",
    "def propogateForward(row,thetas):\n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    row = np.insert(row,0,1)\n",
    "    z2 = theta1.dot(row)\n",
    "    a2 = expit(z2)\n",
    "    a2 = np.insert(a2,0,1) #Add the bias unit\n",
    "\n",
    "    \n",
    "    \n",
    "    z3 = theta2.dot(a2)\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "    classes = np.arange(1,11) \n",
    "    rVal = classes[np.argmax(a3)]\n",
    "\n",
    "    return rVal    \n",
    "    \n",
    "def computeAccuracy(thetas, X, y):\n",
    "    m = X.shape[0]\n",
    "    numCorrect = 0\n",
    "    #costFunction(thetas, Xtest, yTest, 0)\n",
    "    for i in range(m):\n",
    "        if int(propogateForward(X[i],thetas) == int(y[i])):\n",
    "            numCorrect += 1\n",
    "    print(\"Training set accuracy is %.2f\" %(100*(numCorrect/m)))\n",
    "computeAccuracy(trainThetas,Xtest,yTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# redoing nn but using pytorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "dataFile = \"data/ex4data1.mat\"\n",
    "data = scipy.io.loadmat(dataFile)\n",
    "\n",
    "\n",
    "# X has 5000 examples and 400 features making it 5000x400\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "X = torch.tensor(X, dtype = torch.float)\n",
    "y = torch.tensor(y, dtype = torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#0 Loss: 33.7147102355957\n",
      "#1 Loss: 38.5\n",
      "#2 Loss: 38.5\n",
      "#3 Loss: 38.5\n",
      "#4 Loss: 38.5\n",
      "#5 Loss: 38.5\n",
      "#6 Loss: 38.5\n",
      "#7 Loss: 38.5\n",
      "#8 Loss: 38.5\n",
      "#9 Loss: 38.5\n",
      "#10 Loss: 38.5\n",
      "#11 Loss: 38.5\n",
      "#12 Loss: 38.5\n",
      "#13 Loss: 38.5\n",
      "#14 Loss: 38.5\n",
      "#15 Loss: 38.5\n",
      "#16 Loss: 38.5\n",
      "#17 Loss: 38.5\n",
      "#18 Loss: 38.5\n",
      "#19 Loss: 38.5\n",
      "#20 Loss: 38.5\n",
      "#21 Loss: 38.5\n",
      "#22 Loss: 38.5\n",
      "#23 Loss: 38.5\n",
      "#24 Loss: 38.5\n",
      "#25 Loss: 38.5\n",
      "#26 Loss: 38.5\n",
      "#27 Loss: 38.5\n",
      "#28 Loss: 38.5\n",
      "#29 Loss: 38.5\n",
      "#30 Loss: 38.5\n",
      "#31 Loss: 38.5\n",
      "#32 Loss: 38.5\n",
      "#33 Loss: 38.5\n",
      "#34 Loss: 38.5\n",
      "#35 Loss: 38.5\n",
      "#36 Loss: 38.5\n",
      "#37 Loss: 38.5\n",
      "#38 Loss: 38.5\n",
      "#39 Loss: 38.5\n",
      "#40 Loss: 38.5\n",
      "#41 Loss: 38.5\n",
      "#42 Loss: 38.5\n",
      "#43 Loss: 38.5\n",
      "#44 Loss: 38.5\n",
      "#45 Loss: 38.5\n",
      "#46 Loss: 38.5\n",
      "#47 Loss: 38.5\n",
      "#48 Loss: 38.5\n",
      "#49 Loss: 38.5\n",
      "#50 Loss: 38.5\n",
      "#51 Loss: 38.5\n",
      "#52 Loss: 38.5\n",
      "#53 Loss: 38.5\n",
      "#54 Loss: 38.5\n",
      "#55 Loss: 38.5\n",
      "#56 Loss: 38.5\n",
      "#57 Loss: 38.5\n",
      "#58 Loss: 38.5\n",
      "#59 Loss: 38.5\n",
      "#60 Loss: 38.5\n",
      "#61 Loss: 38.5\n",
      "#62 Loss: 38.5\n",
      "#63 Loss: 38.5\n",
      "#64 Loss: 38.5\n",
      "#65 Loss: 38.5\n",
      "#66 Loss: 38.5\n",
      "#67 Loss: 38.5\n",
      "#68 Loss: 38.5\n",
      "#69 Loss: 38.5\n",
      "#70 Loss: 38.5\n",
      "#71 Loss: 38.5\n",
      "#72 Loss: 38.5\n",
      "#73 Loss: 38.5\n",
      "#74 Loss: 38.5\n",
      "#75 Loss: 38.5\n",
      "#76 Loss: 38.5\n",
      "#77 Loss: 38.5\n",
      "#78 Loss: 38.5\n",
      "#79 Loss: 38.5\n",
      "#80 Loss: 38.5\n",
      "#81 Loss: 38.5\n",
      "#82 Loss: 38.5\n",
      "#83 Loss: 38.5\n",
      "#84 Loss: 38.5\n",
      "#85 Loss: 38.5\n",
      "#86 Loss: 38.5\n",
      "#87 Loss: 38.5\n",
      "#88 Loss: 38.5\n",
      "#89 Loss: 38.5\n",
      "#90 Loss: 38.5\n",
      "#91 Loss: 38.5\n",
      "#92 Loss: 38.5\n",
      "#93 Loss: 38.5\n",
      "#94 Loss: 38.5\n",
      "#95 Loss: 38.5\n",
      "#96 Loss: 38.5\n",
      "#97 Loss: 38.5\n",
      "#98 Loss: 38.5\n",
      "#99 Loss: 38.5\n",
      "#100 Loss: 38.5\n",
      "#101 Loss: 38.5\n",
      "#102 Loss: 38.5\n",
      "#103 Loss: 38.5\n",
      "#104 Loss: 38.5\n",
      "#105 Loss: 38.5\n",
      "#106 Loss: 38.5\n",
      "#107 Loss: 38.5\n",
      "#108 Loss: 38.5\n",
      "#109 Loss: 38.5\n",
      "#110 Loss: 38.5\n",
      "#111 Loss: 38.5\n",
      "#112 Loss: 38.5\n",
      "#113 Loss: 38.5\n",
      "#114 Loss: 38.5\n",
      "#115 Loss: 38.5\n",
      "#116 Loss: 38.5\n",
      "#117 Loss: 38.5\n",
      "#118 Loss: 38.5\n",
      "#119 Loss: 38.5\n",
      "#120 Loss: 38.5\n",
      "#121 Loss: 38.5\n",
      "#122 Loss: 38.5\n",
      "#123 Loss: 38.5\n",
      "#124 Loss: 38.5\n",
      "#125 Loss: 38.5\n",
      "#126 Loss: 38.5\n",
      "#127 Loss: 38.5\n",
      "#128 Loss: 38.5\n",
      "#129 Loss: 38.5\n",
      "#130 Loss: 38.5\n",
      "#131 Loss: 38.5\n",
      "#132 Loss: 38.5\n",
      "#133 Loss: 38.5\n",
      "#134 Loss: 38.5\n",
      "#135 Loss: 38.5\n",
      "#136 Loss: 38.5\n",
      "#137 Loss: 38.5\n",
      "#138 Loss: 38.5\n",
      "#139 Loss: 38.5\n",
      "#140 Loss: 38.5\n",
      "#141 Loss: 38.5\n",
      "#142 Loss: 38.5\n",
      "#143 Loss: 38.5\n",
      "#144 Loss: 38.5\n",
      "#145 Loss: 38.5\n",
      "#146 Loss: 38.5\n",
      "#147 Loss: 38.5\n",
      "#148 Loss: 38.5\n",
      "#149 Loss: 38.5\n",
      "#150 Loss: 38.5\n",
      "#151 Loss: 38.5\n",
      "#152 Loss: 38.5\n",
      "#153 Loss: 38.5\n",
      "#154 Loss: 38.5\n",
      "#155 Loss: 38.5\n",
      "#156 Loss: 38.5\n",
      "#157 Loss: 38.5\n",
      "#158 Loss: 38.5\n",
      "#159 Loss: 38.5\n",
      "#160 Loss: 38.5\n",
      "#161 Loss: 38.5\n",
      "#162 Loss: 38.5\n",
      "#163 Loss: 38.5\n",
      "#164 Loss: 38.5\n",
      "#165 Loss: 38.5\n",
      "#166 Loss: 38.5\n",
      "#167 Loss: 38.5\n",
      "#168 Loss: 38.5\n",
      "#169 Loss: 38.5\n",
      "#170 Loss: 38.5\n",
      "#171 Loss: 38.5\n",
      "#172 Loss: 38.5\n",
      "#173 Loss: 38.5\n",
      "#174 Loss: 38.5\n",
      "#175 Loss: 38.5\n",
      "#176 Loss: 38.5\n",
      "#177 Loss: 38.5\n",
      "#178 Loss: 38.5\n",
      "#179 Loss: 38.5\n",
      "#180 Loss: 38.5\n",
      "#181 Loss: 38.5\n",
      "#182 Loss: 38.5\n",
      "#183 Loss: 38.5\n",
      "#184 Loss: 38.5\n",
      "#185 Loss: 38.5\n",
      "#186 Loss: 38.5\n",
      "#187 Loss: 38.5\n",
      "#188 Loss: 38.5\n",
      "#189 Loss: 38.5\n",
      "#190 Loss: 38.5\n",
      "#191 Loss: 38.5\n",
      "#192 Loss: 38.5\n",
      "#193 Loss: 38.5\n",
      "#194 Loss: 38.5\n",
      "#195 Loss: 38.5\n",
      "#196 Loss: 38.5\n",
      "#197 Loss: 38.5\n",
      "#198 Loss: 38.5\n",
      "#199 Loss: 38.5\n",
      "#200 Loss: 38.5\n",
      "#201 Loss: 38.5\n",
      "#202 Loss: 38.5\n",
      "#203 Loss: 38.5\n",
      "#204 Loss: 38.5\n",
      "#205 Loss: 38.5\n",
      "#206 Loss: 38.5\n",
      "#207 Loss: 38.5\n",
      "#208 Loss: 38.5\n",
      "#209 Loss: 38.5\n",
      "#210 Loss: 38.5\n",
      "#211 Loss: 38.5\n",
      "#212 Loss: 38.5\n",
      "#213 Loss: 38.5\n",
      "#214 Loss: 38.5\n",
      "#215 Loss: 38.5\n",
      "#216 Loss: 38.5\n",
      "#217 Loss: 38.5\n",
      "#218 Loss: 38.5\n",
      "#219 Loss: 38.5\n",
      "#220 Loss: 38.5\n",
      "#221 Loss: 38.5\n",
      "#222 Loss: 38.5\n",
      "#223 Loss: 38.5\n",
      "#224 Loss: 38.5\n",
      "#225 Loss: 38.5\n",
      "#226 Loss: 38.5\n",
      "#227 Loss: 38.5\n",
      "#228 Loss: 38.5\n",
      "#229 Loss: 38.5\n",
      "#230 Loss: 38.5\n",
      "#231 Loss: 38.5\n",
      "#232 Loss: 38.5\n",
      "#233 Loss: 38.5\n",
      "#234 Loss: 38.5\n",
      "#235 Loss: 38.5\n",
      "#236 Loss: 38.5\n",
      "#237 Loss: 38.5\n",
      "#238 Loss: 38.5\n",
      "#239 Loss: 38.5\n",
      "#240 Loss: 38.5\n",
      "#241 Loss: 38.5\n",
      "#242 Loss: 38.5\n",
      "#243 Loss: 38.5\n",
      "#244 Loss: 38.5\n",
      "#245 Loss: 38.5\n",
      "#246 Loss: 38.5\n",
      "#247 Loss: 38.5\n",
      "#248 Loss: 38.5\n",
      "#249 Loss: 38.5\n",
      "#250 Loss: 38.5\n",
      "#251 Loss: 38.5\n",
      "#252 Loss: 38.5\n",
      "#253 Loss: 38.5\n",
      "#254 Loss: 38.5\n",
      "#255 Loss: 38.5\n",
      "#256 Loss: 38.5\n",
      "#257 Loss: 38.5\n",
      "#258 Loss: 38.5\n",
      "#259 Loss: 38.5\n",
      "#260 Loss: 38.5\n",
      "#261 Loss: 38.5\n",
      "#262 Loss: 38.5\n",
      "#263 Loss: 38.5\n",
      "#264 Loss: 38.5\n",
      "#265 Loss: 38.5\n",
      "#266 Loss: 38.5\n",
      "#267 Loss: 38.5\n",
      "#268 Loss: 38.5\n",
      "#269 Loss: 38.5\n",
      "#270 Loss: 38.5\n",
      "#271 Loss: 38.5\n",
      "#272 Loss: 38.5\n",
      "#273 Loss: 38.5\n",
      "#274 Loss: 38.5\n",
      "#275 Loss: 38.5\n",
      "#276 Loss: 38.5\n",
      "#277 Loss: 38.5\n",
      "#278 Loss: 38.5\n",
      "#279 Loss: 38.5\n",
      "#280 Loss: 38.5\n",
      "#281 Loss: 38.5\n",
      "#282 Loss: 38.5\n",
      "#283 Loss: 38.5\n",
      "#284 Loss: 38.5\n",
      "#285 Loss: 38.5\n",
      "#286 Loss: 38.5\n",
      "#287 Loss: 38.5\n",
      "#288 Loss: 38.5\n",
      "#289 Loss: 38.5\n",
      "#290 Loss: 38.5\n",
      "#291 Loss: 38.5\n",
      "#292 Loss: 38.5\n",
      "#293 Loss: 38.5\n",
      "#294 Loss: 38.5\n",
      "#295 Loss: 38.5\n",
      "#296 Loss: 38.5\n",
      "#297 Loss: 38.5\n",
      "#298 Loss: 38.5\n",
      "#299 Loss: 38.5\n",
      "#300 Loss: 38.5\n",
      "#301 Loss: 38.5\n",
      "#302 Loss: 38.5\n",
      "#303 Loss: 38.5\n",
      "#304 Loss: 38.5\n",
      "#305 Loss: 38.5\n",
      "#306 Loss: 38.5\n",
      "#307 Loss: 38.5\n",
      "#308 Loss: 38.5\n",
      "#309 Loss: 38.5\n",
      "#310 Loss: 38.5\n",
      "#311 Loss: 38.5\n",
      "#312 Loss: 38.5\n",
      "#313 Loss: 38.5\n",
      "#314 Loss: 38.5\n",
      "#315 Loss: 38.5\n",
      "#316 Loss: 38.5\n",
      "#317 Loss: 38.5\n",
      "#318 Loss: 38.5\n",
      "#319 Loss: 38.5\n",
      "#320 Loss: 38.5\n",
      "#321 Loss: 38.5\n",
      "#322 Loss: 38.5\n",
      "#323 Loss: 38.5\n",
      "#324 Loss: 38.5\n",
      "#325 Loss: 38.5\n",
      "#326 Loss: 38.5\n",
      "#327 Loss: 38.5\n",
      "#328 Loss: 38.5\n",
      "#329 Loss: 38.5\n",
      "#330 Loss: 38.5\n",
      "#331 Loss: 38.5\n",
      "#332 Loss: 38.5\n",
      "#333 Loss: 38.5\n",
      "#334 Loss: 38.5\n",
      "#335 Loss: 38.5\n",
      "#336 Loss: 38.5\n",
      "#337 Loss: 38.5\n",
      "#338 Loss: 38.5\n",
      "#339 Loss: 38.5\n",
      "#340 Loss: 38.5\n",
      "#341 Loss: 38.5\n",
      "#342 Loss: 38.5\n",
      "#343 Loss: 38.5\n",
      "#344 Loss: 38.5\n",
      "#345 Loss: 38.5\n",
      "#346 Loss: 38.5\n",
      "#347 Loss: 38.5\n",
      "#348 Loss: 38.5\n",
      "#349 Loss: 38.5\n",
      "#350 Loss: 38.5\n",
      "#351 Loss: 38.5\n",
      "#352 Loss: 38.5\n",
      "#353 Loss: 38.5\n",
      "#354 Loss: 38.5\n",
      "#355 Loss: 38.5\n",
      "#356 Loss: 38.5\n",
      "#357 Loss: 38.5\n",
      "#358 Loss: 38.5\n",
      "#359 Loss: 38.5\n",
      "#360 Loss: 38.5\n",
      "#361 Loss: 38.5\n",
      "#362 Loss: 38.5\n",
      "#363 Loss: 38.5\n",
      "#364 Loss: 38.5\n",
      "#365 Loss: 38.5\n",
      "#366 Loss: 38.5\n",
      "#367 Loss: 38.5\n",
      "#368 Loss: 38.5\n",
      "#369 Loss: 38.5\n",
      "#370 Loss: 38.5\n",
      "#371 Loss: 38.5\n",
      "#372 Loss: 38.5\n",
      "#373 Loss: 38.5\n",
      "#374 Loss: 38.5\n",
      "#375 Loss: 38.5\n",
      "#376 Loss: 38.5\n",
      "#377 Loss: 38.5\n",
      "#378 Loss: 38.5\n",
      "#379 Loss: 38.5\n",
      "#380 Loss: 38.5\n",
      "#381 Loss: 38.5\n",
      "#382 Loss: 38.5\n",
      "#383 Loss: 38.5\n",
      "#384 Loss: 38.5\n",
      "#385 Loss: 38.5\n",
      "#386 Loss: 38.5\n",
      "#387 Loss: 38.5\n",
      "#388 Loss: 38.5\n",
      "#389 Loss: 38.5\n",
      "#390 Loss: 38.5\n",
      "#391 Loss: 38.5\n",
      "#392 Loss: 38.5\n",
      "#393 Loss: 38.5\n",
      "#394 Loss: 38.5\n",
      "#395 Loss: 38.5\n",
      "#396 Loss: 38.5\n",
      "#397 Loss: 38.5\n",
      "#398 Loss: 38.5\n",
      "#399 Loss: 38.5\n"
     ]
    }
   ],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        self.inputSize = 400\n",
    "        self.hiddenSize = 25\n",
    "        self.outputSize = 10\n",
    "        \n",
    "        self.W1 = torch.randn(self.inputSize, self.hiddenSize)\n",
    "        self.W2 = torch.randn(self.hiddenSize, self.outputSize)\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.z2 = torch.matmul(X,self.W1)\n",
    "        self.a2 = expit(self.z2)\n",
    "        self.z3 = torch.matmul(self.a2, self.W2)\n",
    "        self.a3 = expit(self.z3)\n",
    "        \n",
    "        return self.a3\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def backward(self, X, y, out):\n",
    "        self.out_error = y - out\n",
    "        self.out_delta = self.out_error * self.sigmoidPrime(out)\n",
    "        self.z2_error = torch.matmul(self.out_delta, torch.t(self.W2))\n",
    "        self.z2_delta = self.z2_error * self.sigmoidPrime(self.z2)\n",
    "        self.W1 += torch.matmul(torch.t(X), self.z2_delta)\n",
    "        self.W2 += torch.matmul(torch.t(self.z2), self.out_delta)\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        out = self.forward(X)\n",
    "        self.backward(X, y, out)\n",
    "    \n",
    "    def saveWeights(self, model):\n",
    "        torch.save(model, \"NN\")\n",
    "        \n",
    "    def predict(self):\n",
    "        return 1\n",
    "        \n",
    "        \n",
    "        \n",
    "NN = Neural_Network()\n",
    "for i in range(400):  # trains the NN 1,000 times\n",
    "    print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y - NN(X))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X, y)\n",
    "    \n",
    "NN.saveWeights(NN)\n",
    "               \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
