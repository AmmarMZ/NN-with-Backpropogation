{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "import math\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function\n",
    "import array\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = \"data/ex4data1.mat\"\n",
    "data = scipy.io.loadmat(dataFile)\n",
    "\n",
    "\n",
    "# X has 5000 examples and 400 features making it 5000x400\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# training set / test set code\n",
    "# shuffling data set because later on pd.get_dummies needs to identify 10 unique elements but can't if the data only contains \n",
    "# classifications for the first 6 digits\n",
    "combined = np.append(X, y, axis=1)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "yComb = combined[:,400]\n",
    "yComb = yComb.reshape(len(yComb),1)\n",
    "\n",
    "Xcomb = combined[:,0:400]\n",
    "Xcomb = Xcomb.reshape(len(Xcomb),400)\n",
    "\n",
    "# will be using 70% of data to train hence 3500 x 400\n",
    "Xtrain = Xcomb[0:3500,:]\n",
    "Xtrain = np.insert(Xtrain, 0, 1, axis=1)\n",
    "yTrain = yComb[0:3500]\n",
    "\n",
    "# then the test set will be 1500x400\n",
    "Xtest = Xcomb[3500:5001,:]\n",
    "yTest = yComb[3500:5000]\n",
    "\n",
    "#end of train/test set code\n",
    "\n",
    "X = np.insert(X, 0, 1, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from ex3\n",
    "def reshapeRow(row):\n",
    "    \"\"\"\n",
    "    @param {row} 1 x 401 matrix since an image of a digit is 20x20 + 1 that was added as a bias\n",
    "    Function that takes in the pixel intensity values and puts it into a 20x20 square \n",
    "    \"\"\" \n",
    "    # the [1:] is used to take everything after the 1st index \n",
    "    \n",
    "    return row[1:].reshape(20,20).T\n",
    "\n",
    "def displayData(indiciesToDisplay = None):\n",
    "    \"\"\"\n",
    "    Function that selects 100 random examples for the 5000 we have and organizes\n",
    "    them into a 10x10 matrix\n",
    "    \"\"\"\n",
    "    width = 20\n",
    "    height = 20\n",
    "    numRows = 10\n",
    "    numCols = 10\n",
    "    \n",
    "    if not indiciesToDisplay:\n",
    "        indiciesToDisplay = random.sample(range(X.shape[0]), numRows * numCols)\n",
    "\n",
    "    \n",
    "    bigPicture = np.zeros((height * numRows, width * numCols))\n",
    "    \n",
    "    iRow = 0\n",
    "    iCol = 0\n",
    "\n",
    "    for i in indiciesToDisplay :\n",
    "        if iCol == numCols:\n",
    "            iCol = 0\n",
    "            iRow += 1\n",
    " \n",
    "        curImg = reshapeRow(X[i])\n",
    "        bigPicture[iRow * height :iRow * height + curImg.shape[0], \n",
    "                    iCol * width : iCol * width + curImg.shape[1]] = curImg\n",
    "        iCol += 1 \n",
    "    fig = plt.figure( figsize = (6,6) )\n",
    "    img = scipy.misc.toimage( bigPicture )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "\n",
    "#displayData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFile = \"data/ex4weights.mat\"\n",
    "thetas1 = scipy.io.loadmat(thetaFile)\n",
    "\n",
    "# Theta1.shape = 25 x 401\n",
    "Theta1 = thetas1['Theta1']\n",
    "\n",
    "# Theta2.shape = 10 x 26\n",
    "Theta2 = thetas1['Theta2']\n",
    "\n",
    "thetaUnrolled = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "\n",
    "inputSize = 400\n",
    "hiddenSize = 25\n",
    "outputSize = 10\n",
    "\n",
    "\n",
    "# m = 5000\n",
    "m = X.shape[0]\n",
    "# n = 401\n",
    "n = X.shape[1]\n",
    "\n",
    "aVals = [None] * 3\n",
    "zVals = [None] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following 2 functions are needed because fmin_cg passes in X as an unrolled vector\n",
    "def reshape(X, m, n):\n",
    "    return np.array(X).reshape((m,n))\n",
    "\n",
    "def flatten(X):\n",
    "    return np.array(X.flatten()).reshape((X.shape[0]*(X.shape[1]),1))\n",
    "\n",
    "\n",
    "# used to flatted D1, D2, taken from kaleko github\n",
    "def flattenParams(thetas_list):\n",
    "    input_layer_size = 400\n",
    "    hidden_layer_size = 25\n",
    "    output_layer_size = 10\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def sigmoid(X, theta):\n",
    "    return expit(np.dot(X,theta))\n",
    "\n",
    "def costFunction(thetas, X, y, lmbda):\n",
    "    \n",
    "    X = reshape(X,5000,401)\n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    m = X.shape[0]\n",
    "\n",
    "    a1 = X # 5000 x 401\n",
    "    z2 = theta1.dot(X.T)\n",
    "    \n",
    "    a2 = expit(z2) # 25 x 5000\n",
    "    a2 = np.insert(a2,0,1,axis=0)\n",
    "       \n",
    "    z3 = theta2.dot(a2)\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "    aVals[0] = a1\n",
    "    aVals[1] = a2\n",
    "    aVals[2] = a3\n",
    "    \n",
    "    zVals[0] = z2\n",
    "    zVals[1] = z3\n",
    "     \n",
    "    tempY = pd.get_dummies(y.ravel()).values\n",
    "    \n",
    "    J = (-1/m)*np.sum(np.log(a3.T)*(tempY)+np.log(1-a3).T*(1-tempY)) + \\\n",
    "    (lmbda/(2*m))*(np.sum(np.square(theta1[:,1:])) + np.sum(np.square(theta2[:,1:])))  \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCost():\n",
    "    J = costFunction(thetaUnrolled,X,y,1)\n",
    "    #print(\"Expected value for training set is ~ 1.83\\nActual value %.9f\"%J)\n",
    "\n",
    "    #print(\"Expected value is ~ 0.383\\nActual value %.9f\"%J)\n",
    "testCost()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradientSigmoid(z):\n",
    "    return expit(z)*(1-expit(z))\n",
    "\n",
    "def initRandomThetas():\n",
    "    epsilon = 0.12\n",
    "    theta1Size = (hiddenSize, inputSize + 1)\n",
    "    theta2Size = (outputSize, hiddenSize + 1)\n",
    "    \n",
    "    \n",
    "    theta1 = np.random.rand(*theta1Size).ravel()*2*epsilon - epsilon\n",
    "    theta2 = np.random.rand(*theta2Size).ravel()*2*epsilon - epsilon\n",
    "    \n",
    "    \n",
    "    thetas = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "    return thetas\n",
    "\n",
    "temp = initRandomThetas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 2111. Numerical Gradient = -0.000089. BackProp Gradient = -0.000089.\n",
      "Element: 2628. Numerical Gradient = 0.000064. BackProp Gradient = 0.000064.\n",
      "Element: 4135. Numerical Gradient = 0.000145. BackProp Gradient = 0.000145.\n",
      "Element: 2346. Numerical Gradient = -0.000000. BackProp Gradient = -0.000000.\n",
      "Element: 579. Numerical Gradient = -0.000004. BackProp Gradient = -0.000004.\n",
      "Element: 3898. Numerical Gradient = 0.000070. BackProp Gradient = 0.000070.\n",
      "Element: 276. Numerical Gradient = 0.000115. BackProp Gradient = 0.000115.\n",
      "Element: 4837. Numerical Gradient = -0.000001. BackProp Gradient = -0.000001.\n",
      "Element: 4442. Numerical Gradient = -0.000001. BackProp Gradient = -0.000001.\n",
      "Element: 1530. Numerical Gradient = -0.000215. BackProp Gradient = -0.000215.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def backProp(thetas,X,y, lmbda):\n",
    "    X = reshape(X,5000,401)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    a1 = aVals[0]\n",
    "    a2 = aVals[1].T\n",
    "    a3 = aVals[2].T\n",
    "    \n",
    "    z2 = zVals[0].T\n",
    "    z3 = zVals[1].T\n",
    "    \n",
    "        \n",
    "    D1 = np.zeros((theta1.shape))\n",
    "    D2 = np.zeros((theta2.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1Cur = X[i]\n",
    "        a2Cur = a2[:,1:][i]\n",
    "        z2Cur = z2[i]\n",
    "        a3Cur = a3[i]\n",
    "        z3Cur = z3[i]\n",
    "    \n",
    "        a1Cur = a1Cur.reshape(len(a1Cur),1)\n",
    "        a2Cur = a2Cur.reshape(len(a2Cur),1)\n",
    "        z2Cur = z2Cur.reshape(len(z2Cur),1)\n",
    "        a3Cur = a3Cur.reshape(len(a3Cur),1)\n",
    "        z3Cur = z3Cur.reshape(len(z3Cur),1)\n",
    "        \n",
    "        yT = np.zeros((10,1))\n",
    "        yT[int(y[i]) - 1] = 1\n",
    "                \n",
    "        delta3 = a3Cur - yT\n",
    "        delta2 = theta2[:,1:].T.dot(delta3)*gradientSigmoid(z2Cur)\n",
    "        \n",
    "        \n",
    "        a2Cur = np.insert(a2Cur, 0, 1, axis=0)\n",
    "\n",
    "        D1 = D1 + delta2.dot(a1Cur.T)\n",
    "                \n",
    "        delta3 = delta3.reshape(len(delta3),1)\n",
    "        D2 = D2 + delta3.dot(a2Cur.T)\n",
    "        \n",
    "    D1 = D1/m\n",
    "    D2 = D2/m\n",
    "    \n",
    "    D1[:,1:] = D1[:,1:] + (float(lmbda)/m)*theta1[:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (float(lmbda)/m)*theta2[:,1:]\n",
    "    \n",
    "    return flattenParams([D1, D2]).flatten()\n",
    "       \n",
    "def checkGradient(thetas,D,X,y,lmbda):\n",
    "    epsilon = 0.0001\n",
    "    xT = flatten(X)\n",
    "    n_elems = len(thetas) \n",
    "    thetas = thetas.reshape(len(thetas),1)\n",
    "    #Pick ten random elements, compute numerical gradient, compare to respective D's\n",
    "    for i in range(10):\n",
    "        x = int(np.random.rand()*X.shape[0])\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        epsvec[x] = epsilon\n",
    "        cost_high = costFunction(thetas + epsvec, X, y, lmbda)\n",
    "        cost_low  = costFunction(thetas - epsvec, X, y, lmbda)\n",
    "        mygrad = (cost_high - cost_low) / float(2*epsilon)\n",
    "        print (\"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"%(x,mygrad,D[x]))\n",
    "        \n",
    "\n",
    "Ds = backProp(np.r_[Theta1.ravel(),Theta2.ravel()],X,y,0)\n",
    "checkGradient(thetaUnrolled,Ds,X,y,0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.021647\n",
      "         Iterations: 50\n",
      "         Function evaluations: 149\n",
      "         Gradient evaluations: 149\n",
      "Total time is 49.29 seconds\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "import time\n",
    "def trainNN(lmbda):\n",
    "    generatedThetas = initRandomThetas()\n",
    "    generatedThetas = generatedThetas.reshape(len(generatedThetas),1)\n",
    "    \n",
    "    result = scipy.optimize.fmin_cg(costFunction, fprime=backProp, x0=generatedThetas, \\\n",
    "                               args=(X,y,lmbda),maxiter=50,disp=True,full_output=True)\n",
    "    return result[0]\n",
    "\n",
    "start = time.time()\n",
    "trainThetas = trainNN(0)\n",
    "end = time.time()\n",
    "print(\"Total time is %.2f seconds\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2.37623443e-08 1.85503877e-05 8.21986268e-05 3.31721182e-12\n",
      " 8.35657125e-05 5.14301574e-06 2.03117635e-05 3.37015530e-08\n",
      " 9.27134629e-04 9.99989807e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictNN(row, index):\n",
    "    classes = np.arange(1,11) \n",
    "    a3 = aVals[2].T\n",
    "    a3Row = a3[index]\n",
    "    rVal = classes[np.argmax(a3Row)]\n",
    "    return rVal \n",
    "    \n",
    "\n",
    "def propogateForward(row,thetas):\n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    #row = np.insert(row,0,1)\n",
    "    z2 = theta1.dot(row)\n",
    "    a2 = expit(z2)\n",
    "    a2 = np.insert(a2,0,1) #Add the bias unit\n",
    "\n",
    "    \n",
    "    \n",
    "    z3 = theta2.dot(a2)\n",
    "    a3 = expit(z3)\n",
    "    print(a3)\n",
    "    classes = np.arange(1,11) \n",
    "    rVal = classes[np.argmax(a3)]\n",
    "\n",
    "    return rVal    \n",
    "    \n",
    "def computeAccuracy(thetas, X, y):\n",
    "    m = X.shape[0]\n",
    "    numCorrect = 0\n",
    "    #costFunction(thetas, Xtest, yTest, 0)\n",
    "    for i in range(m):\n",
    "        if int(propogateForward(X[i],thetas) == int(y[i])):\n",
    "            numCorrect += 1\n",
    "    print(\"Training set accuracy is %.2f\" %(100*(numCorrect/m)))\n",
    "#computeAccuracy(trainThetas,Xtest,yTest)\n",
    "propogateForward(X[0],trainThetas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss Value: 0.7132682204246521\n",
      "Loss Value: 0.43087828159332275\n",
      "Loss Value: 0.3406588137149811\n",
      "Loss Value: 0.32258737087249756\n",
      "Loss Value: 0.3252289593219757\n",
      "Loss Value: 0.322465181350708\n",
      "Loss Value: 0.316062331199646\n",
      "Loss Value: 0.3082074522972107\n",
      "Loss Value: 0.2993548512458801\n",
      "Loss Value: 0.29159015417099\n",
      "Loss Value: 0.28428366780281067\n",
      "Loss Value: 0.2760030925273895\n",
      "Loss Value: 0.2676929235458374\n",
      "Loss Value: 0.2589881122112274\n",
      "Loss Value: 0.2498340755701065\n",
      "Loss Value: 0.24113225936889648\n",
      "Loss Value: 0.23262950778007507\n",
      "Loss Value: 0.2240828573703766\n",
      "Loss Value: 0.21572744846343994\n",
      "Loss Value: 0.20759651064872742\n",
      "Loss Value: 0.1996290236711502\n",
      "Loss Value: 0.19194236397743225\n",
      "Loss Value: 0.18462802469730377\n",
      "Loss Value: 0.17770597338676453\n",
      "Loss Value: 0.17113006114959717\n",
      "Loss Value: 0.16483494639396667\n",
      "Loss Value: 0.15882200002670288\n",
      "Loss Value: 0.15311075747013092\n",
      "Loss Value: 0.14768311381340027\n",
      "Loss Value: 0.14253605902194977\n",
      "Loss Value: 0.13767845928668976\n",
      "Loss Value: 0.13309884071350098\n",
      "Loss Value: 0.1287936568260193\n",
      "Loss Value: 0.1247469037771225\n",
      "Loss Value: 0.12094037979841232\n",
      "Loss Value: 0.11735445261001587\n",
      "Loss Value: 0.11397244781255722\n",
      "Loss Value: 0.11077889800071716\n",
      "Loss Value: 0.10775920003652573\n",
      "Loss Value: 0.1048983484506607\n",
      "Loss Value: 0.10218460857868195\n",
      "Loss Value: 0.09960965812206268\n",
      "Loss Value: 0.09716398268938065\n",
      "Loss Value: 0.09483896195888519\n",
      "Loss Value: 0.09262707084417343\n",
      "Loss Value: 0.09051541239023209\n",
      "Loss Value: 0.0884907990694046\n",
      "Loss Value: 0.08653319627046585\n",
      "Loss Value: 0.08460680395364761\n",
      "Loss Value: 0.08266137540340424\n",
      "Loss Value: 0.08079024404287338\n",
      "Loss Value: 0.0790252760052681\n",
      "Loss Value: 0.0772937461733818\n",
      "Loss Value: 0.0756695419549942\n",
      "Loss Value: 0.07411796599626541\n",
      "Loss Value: 0.07263152301311493\n",
      "Loss Value: 0.07119202613830566\n",
      "Loss Value: 0.06979408860206604\n",
      "Loss Value: 0.06843027472496033\n",
      "Loss Value: 0.06710425764322281\n",
      "Loss Value: 0.06582935899496078\n",
      "Loss Value: 0.06461664289236069\n",
      "Loss Value: 0.06346001476049423\n",
      "Loss Value: 0.062345463782548904\n",
      "Loss Value: 0.06126656383275986\n",
      "Loss Value: 0.06022235006093979\n",
      "Loss Value: 0.05921122059226036\n",
      "Loss Value: 0.058233071118593216\n",
      "Loss Value: 0.05728526785969734\n",
      "Loss Value: 0.05636801943182945\n",
      "Loss Value: 0.05547849461436272\n",
      "Loss Value: 0.05461592227220535\n",
      "Loss Value: 0.053780410438776016\n",
      "Loss Value: 0.05296936631202698\n",
      "Loss Value: 0.052182815968990326\n",
      "Loss Value: 0.051419179886579514\n",
      "Loss Value: 0.050678323954343796\n",
      "Loss Value: 0.049959197640419006\n",
      "Loss Value: 0.04926089197397232\n",
      "Loss Value: 0.04858266934752464\n",
      "Loss Value: 0.04792365804314613\n",
      "Loss Value: 0.04728313907980919\n",
      "Loss Value: 0.046659644693136215\n",
      "Loss Value: 0.04605290666222572\n",
      "Loss Value: 0.045461542904376984\n",
      "Loss Value: 0.044886358082294464\n",
      "Loss Value: 0.04432564601302147\n",
      "Loss Value: 0.04377909004688263\n",
      "Loss Value: 0.043246664106845856\n",
      "Loss Value: 0.04272764176130295\n",
      "Loss Value: 0.04222149774432182\n",
      "Loss Value: 0.04172835126519203\n",
      "Loss Value: 0.04124708101153374\n",
      "Loss Value: 0.04077734053134918\n",
      "Loss Value: 0.04031922295689583\n",
      "Loss Value: 0.0398721881210804\n",
      "Loss Value: 0.03943580761551857\n",
      "Loss Value: 0.03901008516550064\n",
      "Loss Value: 0.03859388455748558\n",
      "Loss Value: 0.038186781108379364\n",
      "Training Completed- Total time is 94.555378 seconds\n"
     ]
    }
   ],
   "source": [
    "# redoing nn but using pytorch\n",
    "import torch\n",
    "import torch.tensor as tensor\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "import sys\n",
    "\n",
    "def getExpandedYs(m_torch, y_torch):\n",
    "        \n",
    "    yExpanded = torch.zeros(1,10)    \n",
    "    for i in range(m_torch):\n",
    "        yActual = torch.zeros(1,10)\n",
    "\n",
    "        index = y_torch[i].item()\n",
    "\n",
    "        if (index == 10):\n",
    "            index = 0\n",
    "\n",
    "        yActual[0,int(index)] = 1\n",
    "        if (i == 0):\n",
    "            yExpanded[0,:] = yActual\n",
    "        else:\n",
    "            yExpanded = torch.cat((yExpanded,yActual),0)\n",
    "    return yExpanded\n",
    "\n",
    "dataFile = \"data/ex4data1.mat\"\n",
    "data = scipy.io.loadmat(dataFile)\n",
    "\n",
    "# X has 5000 examples and 400 features making it 5000x400\n",
    "X_torch = data['X']\n",
    "y_torch = data['y']\n",
    "\n",
    "\n",
    "inputSize = 400\n",
    "hiddenSize = 25\n",
    "outputSize = 10\n",
    "\n",
    "X_torch = torch.tensor(X_torch, dtype = torch.float)\n",
    "X_torch = Variable(X_torch, requires_grad = False)\n",
    "\n",
    "y_torch = torch.tensor(y_torch, dtype = torch.float)\n",
    "y_torch = Variable(y_torch, requires_grad = True)\n",
    "\n",
    "m_torch = X_torch.shape[0]\n",
    "\n",
    "modelSeq = torch.nn.Sequential(\n",
    "                    torch.nn.Linear(inputSize, hiddenSize),\n",
    "                    torch.nn.Sigmoid(),\n",
    "                    torch.nn.Linear(hiddenSize, outputSize),\n",
    "                    torch.nn.Sigmoid())\n",
    "                    #torch.nn.Softmax())\n",
    "\n",
    "\n",
    "    \n",
    "modelSeq.parameters()\n",
    "loss_fn = torch.nn.BCELoss()\n",
    "learnRate = 0.01\n",
    "optimizer = torch.optim.Adam(modelSeq.parameters(), lr=learnRate)\n",
    "\n",
    "def train():\n",
    "    for i in range(500):\n",
    "        yActual = torch.zeros(10,1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        yPred = modelSeq(X_torch)\n",
    "        \n",
    "        loss = loss_fn(yPred,getExpandedYs(m_torch, y_torch))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        #with torch.no_grad():\n",
    "            #for param in modelSeq.parameters():\n",
    "                #param -= learnRate * param.grad\n",
    "\n",
    "        if (i % 5 == 0):\n",
    "            print(\"Loss Value \" + str(i) + \": \"  + str(loss.item()))\n",
    "\n",
    "            \n",
    "            \n",
    "            \n",
    "start = time.time()\n",
    "for i in range(1):\n",
    "    train()\n",
    "end = time.time()\n",
    "modelSeq.eval()\n",
    "print(\"Training Completed- Total time is %f seconds\" %(end-start))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.26\n"
     ]
    }
   ],
   "source": [
    "count = 0\n",
    "classes = np.arange(1,11) \n",
    "\n",
    "for i in range(m_torch):\n",
    "    expected = modelSeq(X_torch[i])\n",
    "    #print(str(i) + \"  \" + str(expected))\n",
    "    \n",
    "    index = torch.argmax(expected)\n",
    "    \n",
    "    if (index == 0):\n",
    "        index = 10\n",
    "    \n",
    "    \n",
    "    #if (i % 50 == 0):\n",
    "        #print(\"Actual is: \" + str(index.item()) + \" Expected is : \" + str(y_torch[i].item()))\n",
    "        \n",
    "    if (index == y_torch[i].item()):\n",
    "        count += 1\n",
    "        \n",
    "print(count/m_torch * 100)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
