{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import scipy.io #Used to load the OCTAVE *.mat files\n",
    "import scipy.misc #Used to show matrix as an image\n",
    "import matplotlib.cm as cm #Used to display images in a specific colormap\n",
    "import random #To pick random images to display\n",
    "import math\n",
    "import itertools\n",
    "from scipy.special import expit #Vectorized sigmoid function\n",
    "import array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataFile = \"data/ex4data1.mat\"\n",
    "data = scipy.io.loadmat(dataFile)\n",
    "\n",
    "\n",
    "# X has 5000 examples and 400 features making it 5000x400\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "# training set / test set code\n",
    "# shuffling data set because later on pd.get_dummies needs to identify 10 unique elements but can't if the data only contains \n",
    "# classifications for the first 6 digits\n",
    "combined = np.append(X, y, axis=1)\n",
    "np.random.shuffle(combined)\n",
    "\n",
    "yComb = combined[:,400]\n",
    "yComb = yComb.reshape(len(yComb),1)\n",
    "\n",
    "Xcomb = combined[:,0:400]\n",
    "Xcomb = Xcomb.reshape(len(Xcomb),400)\n",
    "\n",
    "# will be using 70% of data to train hence 3500 x 400\n",
    "Xtrain = Xcomb[0:3500,:]\n",
    "Xtrain = np.insert(Xtrain, 0, 1, axis=1)\n",
    "yTrain = yComb[0:3500]\n",
    "\n",
    "# then the test set will be 1500x400\n",
    "Xtest = Xcomb[3500:5001,:]\n",
    "yTest = yComb[3500:5000]\n",
    "\n",
    "#end of train/test set code\n",
    "\n",
    "X = np.insert(X, 0, 1, axis = 1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taken from ex3\n",
    "def reshapeRow(row):\n",
    "    \"\"\"\n",
    "    @param {row} 1 x 401 matrix since an image of a digit is 20x20 + 1 that was added as a bias\n",
    "    Function that takes in the pixel intensity values and puts it into a 20x20 square \n",
    "    \"\"\" \n",
    "    # the [1:] is used to take everything after the 1st index \n",
    "    \n",
    "    return row[1:].reshape(20,20).T\n",
    "\n",
    "def displayData(indiciesToDisplay = None):\n",
    "    \"\"\"\n",
    "    Function that selects 100 random examples for the 5000 we have and organizes\n",
    "    them into a 10x10 matrix\n",
    "    \"\"\"\n",
    "    width = 20\n",
    "    height = 20\n",
    "    numRows = 10\n",
    "    numCols = 10\n",
    "    \n",
    "    if not indiciesToDisplay:\n",
    "        indiciesToDisplay = random.sample(range(X.shape[0]), numRows * numCols)\n",
    "\n",
    "    \n",
    "    bigPicture = np.zeros((height * numRows, width * numCols))\n",
    "    \n",
    "    iRow = 0\n",
    "    iCol = 0\n",
    "\n",
    "    for i in indiciesToDisplay :\n",
    "        if iCol == numCols:\n",
    "            iCol = 0\n",
    "            iRow += 1\n",
    " \n",
    "        curImg = reshapeRow(X[i])\n",
    "        bigPicture[iRow * height :iRow * height + curImg.shape[0], \n",
    "                    iCol * width : iCol * width + curImg.shape[1]] = curImg\n",
    "        iCol += 1 \n",
    "    fig = plt.figure( figsize = (6,6) )\n",
    "    img = scipy.misc.toimage( bigPicture )\n",
    "    plt.imshow(img,cmap = cm.Greys_r)\n",
    "\n",
    "#displayData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "thetaFile = \"data/ex4weights.mat\"\n",
    "thetas1 = scipy.io.loadmat(thetaFile)\n",
    "\n",
    "# Theta1.shape = 25 x 401\n",
    "Theta1 = thetas1['Theta1']\n",
    "\n",
    "# Theta2.shape = 10 x 26\n",
    "Theta2 = thetas1['Theta2']\n",
    "\n",
    "thetaUnrolled = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "\n",
    "inputSize = 400\n",
    "hiddenSize = 25\n",
    "outputSize = 10\n",
    "\n",
    "\n",
    "# m = 5000\n",
    "m = X.shape[0]\n",
    "# n = 401\n",
    "n = X.shape[1]\n",
    "\n",
    "aVals = [None] * 3\n",
    "zVals = [None] * 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following 2 functions are needed because fmin_cg passes in X as an unrolled vector\n",
    "def reshape(X, m, n):\n",
    "    return np.array(X).reshape((m,n))\n",
    "\n",
    "def flatten(X):\n",
    "    return np.array(X.flatten()).reshape((X.shape[0]*(X.shape[1]),1))\n",
    "\n",
    "\n",
    "# used to flatted D1, D2, taken from kaleko github\n",
    "def flattenParams(thetas_list):\n",
    "    input_layer_size = 400\n",
    "    hidden_layer_size = 25\n",
    "    output_layer_size = 10\n",
    "    \"\"\"\n",
    "    Hand this function a list of theta matrices, and it will flatten it\n",
    "    into one long (n,1) shaped numpy array\n",
    "    \"\"\"\n",
    "    flattened_list = [ mytheta.flatten() for mytheta in thetas_list ]\n",
    "    combined = list(itertools.chain.from_iterable(flattened_list))\n",
    "    assert len(combined) == (input_layer_size+1)*hidden_layer_size + \\\n",
    "                            (hidden_layer_size+1)*output_layer_size\n",
    "    return np.array(combined).reshape((len(combined),1))\n",
    "\n",
    "def sigmoid(X, theta):\n",
    "    return expit(np.dot(X,theta))\n",
    "\n",
    "def costFunction(thetas, X, y, lmbda):\n",
    "    \n",
    "    X = reshape(X,3500,401)\n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    m = X.shape[0]\n",
    "\n",
    "    a1 = X # 5000 x 401\n",
    "    z2 = theta1.dot(X.T)\n",
    "    \n",
    "    a2 = expit(z2) # 25 x 5000\n",
    "    a2 = np.insert(a2,0,1,axis=0)\n",
    "       \n",
    "    z3 = theta2.dot(a2)\n",
    "    a3 = expit(z3)\n",
    "    \n",
    "    aVals[0] = a1\n",
    "    aVals[1] = a2\n",
    "    aVals[2] = a3\n",
    "    \n",
    "    zVals[0] = z2\n",
    "    zVals[1] = z3\n",
    "     \n",
    "    tempY = pd.get_dummies(y.ravel()).values\n",
    "    \n",
    "    J = (-1/m)*np.sum(np.log(a3.T)*(tempY)+np.log(1-a3).T*(1-tempY)) + \\\n",
    "    (lmbda/(2*m))*(np.sum(np.square(theta1[:,1:])) + np.sum(np.square(theta2[:,1:])))  \n",
    "    \n",
    "    return J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testCost():\n",
    "    J = costFunction(thetaUnrolled,Xtrain,yTrain,1)\n",
    "    #print(\"Expected value for training set is ~ 1.83\\nActual value %.9f\"%J)\n",
    "\n",
    "    #print(\"Expected value is ~ 0.383\\nActual value %.9f\"%J)\n",
    "testCost()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10285,)\n"
     ]
    }
   ],
   "source": [
    "def gradientSigmoid(z):\n",
    "    return expit(z)*(1-expit(z))\n",
    "\n",
    "def initRandomThetas():\n",
    "    epsilon = 0.12\n",
    "    theta1Size = (hiddenSize, inputSize + 1)\n",
    "    theta2Size = (outputSize, hiddenSize + 1)\n",
    "    \n",
    "    \n",
    "    theta1 = np.random.rand(*theta1Size).ravel()*2*epsilon - epsilon\n",
    "    theta2 = np.random.rand(*theta2Size).ravel()*2*epsilon - epsilon\n",
    "    \n",
    "    \n",
    "    thetas = np.r_[Theta1.ravel(), Theta2.ravel()]\n",
    "    return thetas\n",
    "\n",
    "temp = initRandomThetas()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element: 1404. Numerical Gradient = 0.000001. BackProp Gradient = 0.000001.\n",
      "Element: 730. Numerical Gradient = 0.000249. BackProp Gradient = 0.000249.\n",
      "Element: 1836. Numerical Gradient = -0.000410. BackProp Gradient = -0.000410.\n",
      "Element: 1223. Numerical Gradient = 0.000000. BackProp Gradient = 0.000000.\n",
      "Element: 3158. Numerical Gradient = -0.000157. BackProp Gradient = -0.000157.\n",
      "Element: 969. Numerical Gradient = 0.000078. BackProp Gradient = 0.000078.\n",
      "Element: 1342. Numerical Gradient = 0.000112. BackProp Gradient = 0.000112.\n",
      "Element: 2933. Numerical Gradient = 0.000089. BackProp Gradient = 0.000089.\n",
      "Element: 2960. Numerical Gradient = -0.000011. BackProp Gradient = -0.000011.\n",
      "Element: 832. Numerical Gradient = -0.000001. BackProp Gradient = -0.000001.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def backProp(thetas,X,y, lmbda):\n",
    "    X = reshape(X,3500,401)\n",
    "    m = X.shape[0]\n",
    "    \n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    a1 = aVals[0]\n",
    "    a2 = aVals[1].T\n",
    "    a3 = aVals[2].T\n",
    "    \n",
    "    z2 = zVals[0].T\n",
    "    z3 = zVals[1].T\n",
    "    \n",
    "        \n",
    "    D1 = np.zeros((theta1.shape))\n",
    "    D2 = np.zeros((theta2.shape))\n",
    "    \n",
    "    for i in range(m):\n",
    "        a1Cur = X[i]\n",
    "        a2Cur = a2[:,1:][i]\n",
    "        z2Cur = z2[i]\n",
    "        a3Cur = a3[i]\n",
    "        z3Cur = z3[i]\n",
    "    \n",
    "        a1Cur = a1Cur.reshape(len(a1Cur),1)\n",
    "        a2Cur = a2Cur.reshape(len(a2Cur),1)\n",
    "        z2Cur = z2Cur.reshape(len(z2Cur),1)\n",
    "        a3Cur = a3Cur.reshape(len(a3Cur),1)\n",
    "        z3Cur = z3Cur.reshape(len(z3Cur),1)\n",
    "        \n",
    "        yT = np.zeros((10,1))\n",
    "        yT[int(y[i]) - 1] = 1\n",
    "                \n",
    "        delta3 = a3Cur - yT\n",
    "        delta2 = theta2[:,1:].T.dot(delta3)*gradientSigmoid(z2Cur)\n",
    "        \n",
    "        \n",
    "        a2Cur = np.insert(a2Cur, 0, 1, axis=0)\n",
    "\n",
    "        D1 = D1 + delta2.dot(a1Cur.T)\n",
    "                \n",
    "        delta3 = delta3.reshape(len(delta3),1)\n",
    "        D2 = D2 + delta3.dot(a2Cur.T)\n",
    "        \n",
    "    D1 = D1/m\n",
    "    D2 = D2/m\n",
    "    \n",
    "    D1[:,1:] = D1[:,1:] + (float(lmbda)/m)*theta1[:,1:]\n",
    "    D2[:,1:] = D2[:,1:] + (float(lmbda)/m)*theta2[:,1:]\n",
    "    \n",
    "    return flattenParams([D1, D2]).flatten()\n",
    "       \n",
    "def checkGradient(thetas,D,X,y,lmbda):\n",
    "    epsilon = 0.0001\n",
    "    xT = flatten(X)\n",
    "    n_elems = len(thetas) \n",
    "    thetas = thetas.reshape(len(thetas),1)\n",
    "    #Pick ten random elements, compute numerical gradient, compare to respective D's\n",
    "    for i in range(10):\n",
    "        x = int(np.random.rand()*X.shape[0])\n",
    "        epsvec = np.zeros((n_elems,1))\n",
    "        epsvec[x] = epsilon\n",
    "        cost_high = costFunction(thetas + epsvec, X, y, lmbda)\n",
    "        cost_low  = costFunction(thetas - epsvec, X, y, lmbda)\n",
    "        mygrad = (cost_high - cost_low) / float(2*epsilon)\n",
    "        print (\"Element: %d. Numerical Gradient = %f. BackProp Gradient = %f.\"%(x,mygrad,D[x]))\n",
    "        \n",
    "\n",
    "Ds = backProp(np.r_[Theta1.ravel(),Theta2.ravel()],Xtrain,yTrain,0)\n",
    "checkGradient(thetaUnrolled,Ds,Xtrain,yTrain,0)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Maximum number of iterations has been exceeded.\n",
      "         Current function value: 0.029367\n",
      "         Iterations: 50\n",
      "         Function evaluations: 133\n",
      "         Gradient evaluations: 133\n",
      "Total time is 34.87 seconds\n"
     ]
    }
   ],
   "source": [
    "import scipy.optimize\n",
    "import time\n",
    "def trainNN(lmbda):\n",
    "    generatedThetas = initRandomThetas()\n",
    "    generatedThetas = generatedThetas.reshape(len(generatedThetas),1)\n",
    "    \n",
    "    result = scipy.optimize.fmin_cg(costFunction, fprime=backProp, x0=generatedThetas, \\\n",
    "                               args=(Xtrain,yTrain,lmbda),maxiter=50,disp=True,full_output=True)\n",
    "    return result[0]\n",
    "\n",
    "start = time.time()\n",
    "trainThetas = trainNN(0)\n",
    "end = time.time()\n",
    "print(\"Total time is %.2f seconds\" %(end - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.14406686e-08 2.77085260e-06 2.77506390e-05 7.56977843e-11\n",
      " 9.69643953e-05 1.15203435e-05 2.34320505e-04 3.76855427e-08\n",
      " 7.98772151e-04 9.99989299e-01]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 444,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def predictNN(row, index):\n",
    "    classes = np.arange(1,11) \n",
    "    a3 = aVals[2].T\n",
    "    a3Row = a3[index]\n",
    "    rVal = classes[np.argmax(a3Row)]\n",
    "    return rVal \n",
    "    \n",
    "\n",
    "def propogateForward(row,thetas):\n",
    "    theta1 = thetas[0:(hiddenSize*(inputSize+1))].reshape(hiddenSize,(inputSize+1))\n",
    "    theta2 = thetas[(hiddenSize*(inputSize+1)):].reshape(10,(hiddenSize+1))\n",
    "    \n",
    "    row = np.insert(row,0,1)\n",
    "    z2 = theta1.dot(row)\n",
    "    a2 = expit(z2)\n",
    "    a2 = np.insert(a2,0,1) #Add the bias unit\n",
    "\n",
    "    \n",
    "    \n",
    "    z3 = theta2.dot(a2)\n",
    "    a3 = expit(z3)\n",
    "    classes = np.arange(1,11) \n",
    "    rVal = classes[np.argmax(a3)]\n",
    "\n",
    "    return rVal    \n",
    "    \n",
    "def computeAccuracy(thetas, X, y):\n",
    "    m = X.shape[0]\n",
    "    numCorrect = 0\n",
    "    #costFunction(thetas, Xtest, yTest, 0)\n",
    "    for i in range(m):\n",
    "        if int(propogateForward(X[i],thetas) == int(y[i])):\n",
    "            numCorrect += 1\n",
    "    print(\"Training set accuracy is %.2f\" %(100*(numCorrect/m)))\n",
    "computeAccuracy(trainThetas,Xtest,yTest)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-36.7358,  20.5157, -25.3529,  -9.4248, -46.0723,  -2.0947,  27.9001,\n",
      "           0.6748,  32.1753, -69.0018]], grad_fn=<MmBackward>)\n",
      "bias.grad before backward\n",
      "None\n",
      "bias.grad after backward\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# redoing nn but using pytorch\n",
    "import torch\n",
    "import torch.tensor as tensor\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import torch.optim\n",
    "import torchvision.models as models\n",
    "from torch.nn.parameter import Parameter\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "\n",
    "dataFile = \"data/ex4data1.mat\"\n",
    "data = scipy.io.loadmat(dataFile)\n",
    "\n",
    "# X has 5000 examples and 400 features making it 5000x400\n",
    "X = data['X']\n",
    "y = data['y']\n",
    "\n",
    "#X = np.insert(X, 0, 1, axis = 1)\n",
    "\n",
    "inputSize = 400\n",
    "hiddenSize = 25\n",
    "outputSize = 10\n",
    "\n",
    "X = torch.tensor(X, dtype = torch.float)\n",
    "X = Variable(X, requires_grad = False)\n",
    "\n",
    "y = torch.tensor(y, dtype = torch.float)\n",
    "y = Variable(y, requires_grad = True)\n",
    "\n",
    "W1 = Variable(torch.randn(inputSize + 1, hiddenSize).type(torch.float), requires_grad = True)\n",
    "W2 = Variable(torch.randn(hiddenSize + 1, outputSize).type(torch.float), requires_grad = True)\n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model,self).__init__()\n",
    "        self.hidden = nn.Linear(inputSize,hiddenSize)\n",
    "        self.output = nn.Linear(hiddenSize,outputSize)\n",
    "        \n",
    "        self.W1 = Parameter(torch.randn(inputSize,hiddenSize))\n",
    "        self.W2 = Parameter(torch.randn(hiddenSize,outputSize))\n",
    "        \n",
    "    def forward(self,row):\n",
    "        a2 = F.linear(torch.t(row),torch.t(self.W1))\n",
    "        a3 = F.linear(a2,torch.t(self.W2))\n",
    "        print(a3)\n",
    "        return a3\n",
    "    \n",
    "    def num_flat_features(self, x):\n",
    "        size = x.size()[1:]  # all dimensions except the batch dimension\n",
    "        num_features = 1\n",
    "        for s in size:\n",
    "            num_features *= s\n",
    "        return num_features\n",
    "\n",
    "nNet = Model()\n",
    "\n",
    "input = X[0]\n",
    "input = tensor(input.numpy().reshape(len(input),1))\n",
    "out = nNet(input)\n",
    "\n",
    "\n",
    "\n",
    "target = torch.randn(10)\n",
    "\n",
    "params = list(nNet.parameters())\n",
    "\n",
    "target = target.view(10,-1)\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "loss = criterion(out,target)\n",
    "\n",
    "\n",
    "\n",
    "#nNet.zero_grad()\n",
    "\n",
    "print('bias.grad before backward')\n",
    "print(nNet.hidden.bias.grad)\n",
    "\n",
    "loss.backward()\n",
    "\n",
    "print('bias.grad after backward')\n",
    "print(nNet.hidden.bias.grad)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#w1 = params[0].detach().numpy()\n",
    "#w2 = params[1].detach().numpy()\n",
    "\n",
    "#w1 = np.insert(w1, 0, 1, axis = 0)\n",
    "#w2 = np.insert(w2, 0, 1, axis = 0)\n",
    "#print(w1.shape)\n",
    "#print(w2.shape)\n",
    "\n",
    "#weights = np.r_[w1.ravel(),w2.ravel()]\n",
    "#computeAccuracy(weights,X,y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'forward'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-187-2a1628013619>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 68\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     69\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'forward'"
     ]
    }
   ],
   "source": [
    "class Neural_Network(nn.Module):\n",
    "    def __init__(self,):\n",
    "        super(Neural_Network, self).__init__()\n",
    "        self.inputSize = 400\n",
    "        self.hiddenSize = 25\n",
    "        self.outputSize = 10\n",
    "        \n",
    "        self.W1 = torch.randn(self.inputSize + 1, self.hiddenSize)\n",
    "        self.W2 = torch.randn(self.hiddenSize + 1, self.outputSize)\n",
    "        \n",
    "        self.D1 = torch.randn(self.inputSize + 1, self.hiddenSize)\n",
    "        self.D2 = torch.randn(self.hiddenSize + 1, self.outputSize)\n",
    "\n",
    "    \n",
    "    def forward(self, row):\n",
    "            \n",
    "        self.z2 = torch.matmul(row,self.W1)\n",
    "        self.a2 = expit(self.z2)\n",
    "        self.a2 = np.insert(self.a2, 0, 1, axis = 0)\n",
    "        \n",
    "        self.z3 = torch.matmul(self.a2, self.W2)\n",
    "        self.a3 = expit(self.z3)\n",
    "              \n",
    "        classes = np.arange(1,11) \n",
    "        rVal = torch.tensor(classes[np.argmax(self.a3.numpy())])\n",
    "        return rVal.item()\n",
    "    \n",
    "    def sigmoid(self, s):\n",
    "        return 1 / (1 + torch.exp(-s))\n",
    "    \n",
    "    def sigmoidPrime(self, s):\n",
    "        return s * (1-s)\n",
    "    \n",
    "    def backward(self, X, y, out, index):\n",
    "        \n",
    "        yT = np.zeros((10,1))\n",
    "        yT[int(y[index]) - 1] = 1\n",
    "        yT = torch.tensor(yT, dtype=torch.float)\n",
    "        \n",
    "        \n",
    "        self.delta3 = out - yT\n",
    "\n",
    "        self.delta2 = torch.matmul(self.W2[:1,:],self.delta3) * self.sigmoidPrime(self.z2)\n",
    "\n",
    "        xTemp = X.numpy().reshape(len(X),1)\n",
    "        xTemp = torch.tensor(xTemp, dtype = torch.float)\n",
    "        self.D1 += torch.matmul(xTemp, self.delta2)\n",
    "        \n",
    "        \n",
    "        tempZ2 = torch.tensor(self.z2.numpy().reshape(len(self.z2),1), dtype = torch.float)\n",
    "        self.D2[1:,:] += torch.matmul(tempZ2, torch.t(self.delta3))\n",
    "    \n",
    "    def train(self, X, y, index):\n",
    "        out = self.forward(X)\n",
    "        self.backward(X, y, out, index)\n",
    "    \n",
    "    def saveWeights(self, model):\n",
    "        self.D1 = self.D1/5000\n",
    "        self.D1 = self.D2/5000\n",
    "        torch.save(model, \"NN\")\n",
    "        \n",
    "    def predict(self):\n",
    "        print (\"Predicted data based on trained weights: \")\n",
    "        print (\"Input (scaled): \\n\" + str(X))\n",
    "        print (\"Output: \\n\" + str(self.forward(X)))\n",
    "        \n",
    "\n",
    "X.forward()\n",
    "X.backward()\n",
    "    \n",
    "NN = Neural_Network()\n",
    "\n",
    "tW1 = NN.W1.numpy()\n",
    "tW2 = NN.W2.numpy()\n",
    "t_theta = np.r_[tW1.ravel(),tW2.ravel()]\n",
    "\n",
    "computeAccuracy(t_theta,X,y)\n",
    "X_t = np.insert(X, 0, 1, axis = 1)\n",
    "for i in range(X_t.shape[0]):  # trains the NN 1,000 times\n",
    "    #print (\"#\" + str(i) + \" Loss: \" + str(torch.mean((y[i] - NN(X_t[i]))**2).detach().item()))  # mean sum squared loss\n",
    "    NN.train(X_t[i], y, i)\n",
    "print(\"Training Done\")\n",
    "    \n",
    "NN.saveWeights(NN)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set accuracy is 16.86\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "computeAccuracy(t_theta,X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
